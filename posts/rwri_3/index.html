<!DOCTYPE html>
<html lang="en">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">




<title>RWRI Day 3 | As it is</title>

<link rel="stylesheet" href="https://jgerardsimcock.github.io//css/styles.css">

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" 
integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.3.1.js" integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60=" crossorigin="anonymous"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/zenburn.min.css" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
<script>hljs.initHighlightingOnLoad();</script>
<script src="https://jgerardsimcock.github.io//js/highlight.js"></script>






<div class="container">
    <nav class="navbar level">
      <div class="navbar-brand">
          <a class="nav-item" href="https://jgerardsimcock.github.io/"><h1 class="title is-3">As it is</h1></a>
      </div>           
      <div class="navbar-menu has-text-centered is-active">
          <div class="navbar-end is-centered">
              
                <a href="https://github.com/jgerardsimcock">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                </a>
              
                <a href="https://www.linkedin.com/in/jgerardsimcock">
                  <span class="icon">
                    <i class="fab fa-linkedin-in"></i>
                  </span>
                </a>
              
                <a href="https://twitter.com/jgerardsimcock">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                </a>
              
                <a href="https://www.instagram.com/_justin_gerard/">
                  <span class="icon">
                    <i class="fab fa-instagram"></i>
                  </span>
                </a>
              
                <a href="https://www.goodreads.com/user/show/14378346-justin-simcock">
                  <span class="icon">
                    <i class="fab fa-goodreads"></i>
                  </span>
                </a>
              
           </div>
      </div>
    </nav>
  </div>
<div class="container">
  <h2 class="subtitle is-6">February 13, 2019</h2>
  <h1 class="subtitle is-size-4-mobile is-size-3-desktop">RWRI Day 3</h1>
  <div class="content">
    <p>Day 3 of RWRI 10 started with a discussion on the insurance industry and tail-risk events.</p>

<p>This is something I worked on <a href="https://rhg.com/"><strong>Rhodium Group</strong></a> and it was very interesting to listen to Arie Haziza&rsquo;s thoughts. In the past, Rhodium had worked with <a href="https://www.rms.com/"><strong>RMS</strong></a>, Azie&rsquo;s employer, so I knew that this was relevant and legit.</p>

<p>Arie gave us a general overview of the models and sets of assumptions that go into modeling outcomes in the insurance business. For mediocrastan events like car accidents, you have high-frequency of incidence, those incidents are uniform and contained, and no correlation between events. Each exposure is independently exposed to loss. Importantly, the policy holder has skin in the game and participates in the outcome. With the law of large numbers the sample gets larger and larger, the distribution tighter and less dispersed. In this world, you can make safe bets on the total losses due to incidents. In this way, the portfolio is a portfolio of risks, the insurance company collects premium every month and pays out claims when an incident occurs. The chance of ruin for these companies from this activity is small to non-existent.</p>

<p>However, in extremistan these assumptions do not hold and catastrophic risk is all about extremistan. Arie admitted that the Reinsurance industry relies too heavily on catastrophic model vendors for their estimates of risk. Importantly, policyholders of flood insurance have little ability to affect outcomes other than maybe move or add some protection to their house. Robert Frey mentioned his home is 100 ft above sea level and during Sandy his home was unaffected while those around him were destroyed. Arie showed us a set of model estimates for a CAT 5 event for Louisiana.  The estimates ranged from mid 20&rsquo;s to 60 Billion USD. When Katrina hit, and the losses were finally calculated it was well over $60 Billion.</p>

<p>Arie explained that when you get these events, the losses cascade due to interactions in the system. The propery is damaged and of course that needs to be repaired. However, supplies are also destroyed and prices rise. Labor is short due to dislocation of life so the cost of labor goes up. Everything that works and is priced at normal rates during a normal period stops working or is destroyed and all prices increase due to scarcity. These interactions are what drive losses well beyond estimates.</p>

<p>Of course claims are going up and this is due to the fact that more and more people now live in places that persistent exposure to such events. This was perhaps the most interesting part of the presentation. Arie showed us the historical losses associated with different events. It was indeed clear, the trend is increasing. However, he also indicated that property and people have also increased significantly in vulnerable regions and that if we had the same level of assets then as we do now, the worst losses would have happened then. It really put into perspective the fact that cataclysm may ocurr but your exposure to it is what matters. He told us a story of a <a href="https://en.wikipedia.org/wiki/Tunguska_event"><strong>massive asteroid</strong></a> that hit Siberia in 1908. At the time, nobody lived there and there were no assets that could be destroyed as a result. Today, however, the same event would lead to $1 Trillion in losses. And from that you can only imagine what would happen. That is the kind of event that would start some kind of a chain reaction.</p>

<p>Taleb then jumped into make the case that there are limits to the insurance industry. Mainly, you have to make sure that your counterpary can pay you. He told us a story of the Russian banks selling puts on the Russian ruble. The puts were cheaper than they were in France, so he bought them in Russia and sold them in France. To make sure that he did not get trapped, he bought CDS on the Russian banks. Well the banks went bust and he called in his CDS on the banks. Later he explained that the puts were honored because whoever bought the russian banks book, sent them a check. It sounds like that actually worked and he got the bonus of the puts as well.</p>

<p>Another example he added was during the tech bubble Taleb had noticed that the puts became really cheap on single stocks. It turns out, Dell and Gates were selling puts on their own stock. Had those stocks collapsed and had Gates and Dell been forced to pay out, they would have become insolvent. So you have to ask yourself, who is selling you the option and can they actually fulfill the trade. If not possible then don&rsquo;t do it.</p>

<p>In passing a Bitcoin discussion emerged and Taleb made a comment that Bitcoin price has an infinite mean and infinite variance. I did not really understand. He then added there are two kinds of stupid. Those who are saying its going to zero and those who say its going all the way up to&hellip;pick a number. Their is no evidence that the data is stable, there is not enough data. For bitcoin to matter, you need 10^15 currencies to make any sense of bitcoin. To make claims about bitcoin as a currency, you&rsquo;d need alot of data on currencies. We may get there. He drew a similarity with the IQ and wealth debate. If you want to map wealth to mapt to performance/IQ you&rsquo;d need similar planets to make a valid comparison. You need a lot of data to make, confidently, some of the claims that some forecasters make so flippantly.</p>

<p>Complexity and Agent based systems was the next lecture, given by <a href="https://twitter.com/financequant"><strong>Robert Frey</strong></a>. Computation is performed via Cellular Automata.  All processes in nature can be viewed as computation and all processes in nature can be viewed as recursions. The statement was made that at its deepest level the universe is discrete. I guess what this means is that we only have one instance of it. But maybe not. The cellular automata iterate through each time step according to local rules. This leads to the theorem of computational irreducibility. You have to go through every step of a process to get the &lsquo;result&rsquo;. And in this framework, a result does not really have much meaning as the patterns are emergent and not designed a priori. In the aggregate, the cellular automata are capable of complex, semi-regular, seemingly purposeful behaviour. This seems like a kind of <a href="https://en.wikipedia.org/wiki/Ludic_fallacy"><strong>ludic fallacy</strong></a> that we may be falling into when we look at a system and, mistakenly, infer intent.</p>

<p>Stephen Wolfram studied cellular automata in his book <a href="https://www.wolframscience.com/"><strong>New Kind of Science</strong></a>. In it, he identified 4 classes of CA.
* Class one: everything converges to all black
* Class two: convergence to stripes
* Class three: Random states but locally recognizable structures
* Class four: Complex structures, interact in apparently unpredictable ways, computationally irreducible systems</p>

<p><a href="https://en.wikipedia.org/wiki/Rule_110"><strong>Rule 110</strong></a> was mentioned as being a canonical example of a class 4 CA. A system like Rule 110, you have a <a href="https://en.wikipedia.org/wiki/Turing_completeness"><strong>Turing complete</strong></a> computational system. These are discrete systems as opposed to continuous systems and we can see them all over nature.</p>

<p>Population can be modeled as a discrete system with differential equations and using a logistic function to get the population growth rate. As we use different population rates, we get different population states sometimes reaching a kind of equilibrium and sometimes becoming very unstable for different vales of the rate, r. A Lorenz weather system is another system. Tiny differences in initial conditions will lead to drastic shifts in state through time. In genetics we also see recursive systems, where a gene will behave based on the cells next to it. Even though the behavior is local to the gene the interactions generate system-level complexity.</p>

<p>An agent based model is where you have a bunch of cellular automata interacting in a simulation that leads to collective behavior. Agents are governed by simple local rules and from these interactions, complex behavior emerges. Ant colonies are good example of this. We used John Conway&rsquo;s <a href="https://playgameoflife.com/"><strong>the game of life</strong></a> to demonstrate this.</p>

<p>Robert made reference to an <a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games"><strong>infinite game</strong></a>. I was thinking this too. You never really know where you are in the game, each successive step sets up the conditions for the next step and the next step and so on. There is no order other than the emergent order. To me this is an invocation to live in the day, live this day, this moment, the future will emerge and you will be ok, if you are robust against fat-tails. Trying to plan for the next set of events feels foolish, you want to plan to survive for the next iteration that may be totally unexpected.</p>

<p>Robert ended his session with the Parable of the Deszertularo people. These people live in desert oases and there are two types of people: Stayers and Roamers. No long-term cultural development nor infrastructure can be created without the Stayers. No cultural exchange would occur and development would stagnate without the Roamers. The Roamers make little contribution to knowledge and infrastructure. Without Roamers, the Stayers would die off when the oasis dries up. So the Roamers take massive risk and go to search for a new oasis. When they do, they extend the culture. Usually they die in the search. Without both people, the civilization of the desert would die. Without stayers no development, without Roamers the civilization would be isolated and destined to fail. This example showed how you can have two types of agents in a system that operate according to their own rules and through time a given system will evolve based on the interactions of agents in the system.</p>

<p>Robert made the assertion that alternatively, continuous top-down systems will fail when meeting real-world stresses. I think what he meant by this is that top-down systems are managed within some kind of engineering framework that hopes to meet forecasted targets. Those targets are usually constructed through some kind of regression procedure that relies on low-dimensionality, continuous mathematics. These forecasts and systems created to reach those forecasted goals usually fail because low-dimensionality, continuous mathematics cannot estimate the consequences of all the non-linear interactions that emerge. I think the <a href="https://en.wikipedia.org/wiki/N-body_problem"><strong>n-body problem</strong></a> is a version of this.</p>

<p>Next <a href="https://twitter.com/trishankkarthik"><strong>Trishank Karthik</strong></a> spoke about some fairly abstract ideas in computer science. He wanted us to know that computing has existed for thousands of years. Ancient civilizations had constructed architecture to compute the movement of the stars, the greeks created the <a href="https://en.wikipedia.org/wiki/Antikythera_mechanism"><strong>Antikythera mechanism</strong></a>. The idea of an universal computer is recent. Trishank makes the claim that computation may be the mother of all philosophy.</p>

<p>In computer science there are things that are undecideable and intractable. <a href="https://en.wikipedia.org/wiki/Undecidable_problem"><strong>Undecidable problems</strong></a> are those for which it will be impossible to construct an algorithm that will always lead to a yes or no answer. <a href="https://en.wikipedia.org/wiki/Halting_problem"><strong>The Halting Problem</strong></a> is an example of this. The Halting problem: it can be proven that there is no algorithm that correctly determines whether arbitrary programs eventually halt when run. Trishank makes the case that the best way to prove whether something is true or false is through contradiction. There are problems that can theoretically be solved but to solve them would actually take too many resources to be useful. These problems are known as <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory#Intractability"><strong>intractabile</strong></a>.</p>

<p>He continued with a story of <a href="https://en.wikipedia.org/wiki/Georg_Cantor"><strong>Cantors infinite sets and Hotel infinity</strong></a>. The real numbers are those between 0 and 1. There are an infinite set of points between 0 and 1. There are infinite set of points between each of the nature numbers 1,2,3,&hellip; These were controversial ideas. Cantors <a href="https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument"><strong>diagonal argument</strong></a> was also controversial. There was a German mathematician named David Hilbert who came up with a list of <a href="https://en.wikipedia.org/wiki/Hilbert%27s_problems"><strong>unresolved math problems</strong></a>. Hilbert wanted to come up with a <a href="https://en.wikipedia.org/wiki/Hilbert%27s_program"><strong>formal system</strong></a> to solve outstanding problems in mathematics. One of those problems was Cantor&rsquo;s inifinite sets. <a href="https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems">Kurt Godel</a> basically proved that such a goal was impossible and that every axiomatic system would have limitations. He used a diagonal argument, similar to Cantor, to support his theorems.</p>

<p><a href="https://en.wikipedia.org/wiki/Alan_Turing"><strong>Alan Turing</strong></a> and <a href="https://en.wikipedia.org/wiki/Alonzo_Church"><strong>Alonzo Church</strong></a> helped move the field of computation forward. Church developed <a href="https://en.wikipedia.org/wiki/Lambda_calculus"><strong>Lambda Calculus</strong></a> and Turing invented the concept of <a href="https://en.wikipedia.org/wiki/Turing_machine"><strong>Universal Turing machine</strong></a>. The universality part means that some machine can be used to simulate all machines, even itself. The universal machine is simply an interpreter that can take different rules and inputs.</p>

<p>These machines encounter problems of complexity and a self-referencing type problems. If a Turing machine is being used to model a system of a certain complexity and if the model is trying to model something as complex as itself, its not obvious that this system would not simply run infinitely in a loop trying to solve itself. These are intractable problems and have led to the study of algorithms and Big-O analysis. There are many many problems that cannot be verified quickly.</p>

<p>I wondered what the connection with fat-tails, black-swans, anti-fragility and the like was with this discussion. On further meditation it seems that it is related to Trishanks comment about contradiction. Some problems can never be proven because it will take us literally infinite time to solve. However, some problems can be efficiently solved with a contradiction. Negative empiricism or a single data point disproving a hypothesis is better than an infinite number of data points to support it. The entire orientation of problem solving is turned on it&rsquo;s head. Through negative evaluation, we can start to discard possibilities. This is the <a href="https://en.wikipedia.org/wiki/Karl_Popper"><strong>Popperian approach</strong></a>. Actually, as I reread the notes, I see that Taleb is suggesting iterating through the distributions to evaluate which class of phenomena we may be observing. If one data point invalidates a guassian distribution, then your process is not gaussian. Keep going through all the distributions to reduce the possible set of distributions that approximate your process.</p>

<p>The last session of the day was another discussion with Robert Frey on market drawdowns. He took 180 years worth of market data and wanted to see if market drawdowns could be modeled and analyzed. He wanted to understand how stable drawdowns are throughout history. What can we learn from examining the drawdowns of an important market over an extended period? Drawdowns are important to understand since the recovery from such events requires <a href="https://www.turtletrader.com/recovery/"><strong>returns that are larger than the loss</strong></a>.</p>

<p>Frey defines drawdowns as a drop of 20%. It turns out that 75% of the time the market is in a drawdown stat and more than half the time in a major drawdown. Frey assumed that the return process consisted of <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables"><strong>IID random variables</strong></a>. He used the <a href="https://en.wikipedia.org/wiki/Cauchy_distribution"><strong>Cauchy distribution</strong></a> to examine drawdowns from pre and post-depression. He used a technique called <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating"><strong>bootstrap aggregating</strong></a> (or bagging) to simulate a lots of different market return processes. His bagging procedure adds theoretical tail events to the empirical data.</p>

<p>What we learned through this analysis is that if you take the very long historical view, certain market characteristics are stable despite massive tech, government and social changes. Also, market max drawdown is in a higher state of time than most people would think. In this analysis, the Great Depression was not even a surprise. The same for 2008 or any other market drawdown. He performed a similar analysis on the FTSE, the main UK index, and found similar results. He mentioned that he&rsquo;d been working on a <a href="https://en.wikipedia.org/wiki/Markov_model"><strong>markov model</strong></a> for market max drawdowns but that nothing convincing had emerged yet. <a href="https://twitter.com/ashokatluri"><strong>Ashok Atluri</strong></a> had a very interesting question. It was about the influence of governments on markets. He suggested that if the market&rsquo;s behavior is the same across different social, tech and government shifts then can we claim that government has any influence on the market? If not, it is a reason to not give two fucks about regulation. For me as a libtertarian, this goes against my bias that government intervention is a net negative for the market. However, if we believe the narrative, that regulation has increased over time, then this form of intervention does not seem to have really had any affect on market behavior. I am not totally certain this is the final conclusion from this but it has me thinking.</p>

<p>Finally, Robert made the assertion that a good statistical estimator is one where, if you have large additions of data, it will not affect the distribution. This makes sense. It&rsquo;s like your frame of reference is never surprised by additional information. It is a world-view in which anything is possible.</p>

  </div>
</div>
<div class="container has-text-centered">
    
</div>

<div class="container has-text-centered">
  
</div>
<section class="section">
  <div class="container has-text-centered">
    <p>&copy; <a href="https://github.com/jgerardsimcock">Simcock</a> 2018</p>
  </div>
</section>


